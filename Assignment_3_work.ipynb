{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Assignment_3-work.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thienan95/assignment/blob/master/Assignment_3_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcD2AovkvsnQ",
        "colab_type": "text"
      },
      "source": [
        "[Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
        "======\n",
        "\n",
        "## Data Set\n",
        "\n",
        "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
        "\n",
        "## File descriptions\n",
        "\n",
        "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.\n",
        "## Data fields\n",
        "\n",
        "* id - Unique ID of each review\n",
        "* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
        "* review - Text of the review\n",
        "\n",
        "## Objective\n",
        "Objective of this dataset is base on **review** we predict **sentiment** (positive or negative) so X is **review** column and y is **sentiment** column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Qxaay0vsnT",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load Dataset\n",
        "we only forcus on \"labeledTrainData.csv\" file\n",
        "\n",
        "Let's first of all have a look at the data.\n",
        "\n",
        "[Click here to download dataset](https://s3-ap-southeast-1.amazonaws.com/ml101-khanhnguyen/week3/assignment/labeledTrainData.tsv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-qxWuJ3vsnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas, numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqRxldVDvsnZ",
        "colab_type": "code",
        "outputId": "c1ae047b-4e3f-4b86-de80-c7030a6ba0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Read dataset with extra params sep='\\t', encoding=\"latin-1\"\n",
        "path = '/content/labeledTrainData.tsv'\n",
        "sentiment = pd.read_csv(path,sep='\\t', encoding = 'latin-1')\n",
        "sentiment.sample(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18347</th>\n",
              "      <td>6307_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I had never heard of this flick despite the co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21598</th>\n",
              "      <td>9371_7</td>\n",
              "      <td>1</td>\n",
              "      <td>One thing I always liked about Robert Ludlum t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11378</th>\n",
              "      <td>1215_10</td>\n",
              "      <td>1</td>\n",
              "      <td>This is my favorite of the three care bears mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>11950_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I went into this movie knowing nothing about i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10567</th>\n",
              "      <td>5722_1</td>\n",
              "      <td>0</td>\n",
              "      <td>First I'd like to excuse my bad English.&lt;br /&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>11327_4</td>\n",
              "      <td>0</td>\n",
              "      <td>Set in a post-apocalyptic environment, cyborgs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7858</th>\n",
              "      <td>8045_8</td>\n",
              "      <td>1</td>\n",
              "      <td>I finally got myself set up on mail order DVD ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13766</th>\n",
              "      <td>7512_1</td>\n",
              "      <td>0</td>\n",
              "      <td>I had to walk out of the theater. After an hou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2808</th>\n",
              "      <td>265_1</td>\n",
              "      <td>0</td>\n",
              "      <td>The plot of this terrible film is so convolute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17790</th>\n",
              "      <td>9615_1</td>\n",
              "      <td>0</td>\n",
              "      <td>It seems evident from this adaptation that he ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16393</th>\n",
              "      <td>10591_3</td>\n",
              "      <td>0</td>\n",
              "      <td>We open with Colonel Luc Deveraux (Van Damme),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12090</th>\n",
              "      <td>3901_10</td>\n",
              "      <td>1</td>\n",
              "      <td>This video rocked! Eddie is one of the funnies...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20876</th>\n",
              "      <td>10166_7</td>\n",
              "      <td>1</td>\n",
              "      <td>I watched this on an 8 hour flight and (presum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13922</th>\n",
              "      <td>11656_2</td>\n",
              "      <td>0</td>\n",
              "      <td>Scary Movie 2 was a grave disappointment. Simp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22328</th>\n",
              "      <td>2513_4</td>\n",
              "      <td>0</td>\n",
              "      <td>When I first saw this film, I thought it shoul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20321</th>\n",
              "      <td>324_1</td>\n",
              "      <td>0</td>\n",
              "      <td>Worst mistake of my life.&lt;br /&gt;&lt;br /&gt;I picked ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>9338_3</td>\n",
              "      <td>0</td>\n",
              "      <td>Pam Grier stars as Coffy. She's a nurse who se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15147</th>\n",
              "      <td>8985_2</td>\n",
              "      <td>0</td>\n",
              "      <td>Previous comments encouraged me to check this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9972</th>\n",
              "      <td>3379_2</td>\n",
              "      <td>0</td>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24263</th>\n",
              "      <td>9745_2</td>\n",
              "      <td>0</td>\n",
              "      <td>This movie is spoofed in an episode of Mystery...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  sentiment                                             review\n",
              "18347   6307_8          1  I had never heard of this flick despite the co...\n",
              "21598   9371_7          1  One thing I always liked about Robert Ludlum t...\n",
              "11378  1215_10          1  This is my favorite of the three care bears mo...\n",
              "553    11950_8          1  I went into this movie knowing nothing about i...\n",
              "10567   5722_1          0  First I'd like to excuse my bad English.<br />...\n",
              "684    11327_4          0  Set in a post-apocalyptic environment, cyborgs...\n",
              "7858    8045_8          1  I finally got myself set up on mail order DVD ...\n",
              "13766   7512_1          0  I had to walk out of the theater. After an hou...\n",
              "2808     265_1          0  The plot of this terrible film is so convolute...\n",
              "17790   9615_1          0  It seems evident from this adaptation that he ...\n",
              "16393  10591_3          0  We open with Colonel Luc Deveraux (Van Damme),...\n",
              "12090  3901_10          1  This video rocked! Eddie is one of the funnies...\n",
              "20876  10166_7          1  I watched this on an 8 hour flight and (presum...\n",
              "13922  11656_2          0  Scary Movie 2 was a grave disappointment. Simp...\n",
              "22328   2513_4          0  When I first saw this film, I thought it shoul...\n",
              "20321    324_1          0  Worst mistake of my life.<br /><br />I picked ...\n",
              "261     9338_3          0  Pam Grier stars as Coffy. She's a nurse who se...\n",
              "15147   8985_2          0  Previous comments encouraged me to check this ...\n",
              "9972    3379_2          0  I recently bought the DVD, forgetting just how...\n",
              "24263   9745_2          0  This movie is spoofed in an episode of Mystery..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwYzwnavsnd",
        "colab_type": "text"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D55ZoD3svsne",
        "colab_type": "code",
        "outputId": "c90bf907-680f-4960-df3a-df495dcc9b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pIxc9qEvsnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing special characters and \"trash\"\n",
        "import re\n",
        "def preprocessor(text):\n",
        "    # Remove HTML markup\n",
        "    text = re.sub('<[^>]*>', '', text) # Your code here\n",
        "    \n",
        "    # Save emoticons for later appending\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    # Your code here\n",
        "    \n",
        "    # Remove any non-word character and append the emoticons,\n",
        "    # removing the nose character for standarization. Convert to lower case\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxV1Fjusvsnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer and stemming\n",
        "# tokenizer: to break down our twits in individual words\n",
        "# stemming: reducing a word to its root\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "  token = [] \n",
        "  token = text.split()\n",
        "  return token\n",
        "def tokenizer_porter(text):\n",
        "    token = []\n",
        "    token = [porter.stem(word) for word in text.split()]\n",
        "    return token "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbKLFOpyvsnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the dataset in train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = sentiment['review']\n",
        "y = sentiment['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG2XYRwJvsnp",
        "colab_type": "text"
      },
      "source": [
        "## 3. Create Model and Train \n",
        "\n",
        "Using **Pipeline** to concat **tfidf** step and **LogisticRegression** step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls-OVIn3vsnq",
        "colab_type": "code",
        "outputId": "1307134f-efcd-4fd2-c11c-c9ef8692644d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# Import Pipeline, LogisticRegression, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=lambda x: x,\n",
        "    preprocessor=lambda x: x,\n",
        "    token_pattern=None)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "clf = Pipeline([('tfidf', tfidf_vectorizer), ('lr', lr)])\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=<function <lambda> at 0x7fb979d79bf8>,\n",
              "                                 smooth_idf=True, stop_words=None,\n",
              "                                 strip_accents=None, subl...\n",
              "                                 token_pattern=None,\n",
              "                                 tokenizer=<function <lambda> at 0x7fb979d79d90>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('lr',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='warn', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='warn', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pfa_3HPvsnt",
        "colab_type": "text"
      },
      "source": [
        "## 4. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnd_cxHXvsnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Test dataset to evaluate model\n",
        "# classification_report\n",
        "# confusion matrix\n",
        "y_predict = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Tx_zBxvsnw",
        "colab_type": "text"
      },
      "source": [
        "## 5. Export Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2qeos7hK9kv",
        "colab_type": "code",
        "outputId": "7bb388bf-1641-431a-f404-f8cbcb857e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "accuracy_score(y_test, y_predict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.64736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-1UdhAvLGLy",
        "colab_type": "code",
        "outputId": "05d57a94-0379-4fdb-d56b-b834858b4cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "classification_report(y_test, y_predict).split('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '           0       0.65      0.63      0.64      3136',\n",
              " '           1       0.64      0.67      0.65      3114',\n",
              " '',\n",
              " '    accuracy                           0.65      6250',\n",
              " '   macro avg       0.65      0.65      0.65      6250',\n",
              " 'weighted avg       0.65      0.65      0.65      6250',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwzOXdn2vsnx",
        "colab_type": "code",
        "outputId": "b71713ce-4478-48ca-8d38-b36cf5aaec70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Using pickle to export our trained model\n",
        "import pickle\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-857188d66f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logisticRegression.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7fb979d79bf8>: attribute lookup <lambda> on __main__ failed"
          ]
        }
      ]
    }
  ]
}